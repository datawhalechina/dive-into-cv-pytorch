# 5.2 GANå®æˆ˜: æ‰‹å†™æ•°å­—ç”Ÿæˆ

ğŸ‰ğŸ‰ **åœ¨æ¥ä¸‹æ¥çš„å†…å®¹ä¸­ï¼Œæˆ‘ä»¬å°†ç»“åˆä»£ç ï¼Œæ·±å…¥äº†è§£GANçš„æ¨¡å‹æ¶æ„ï¼Œå¹¶åŠ¨æ‰‹å®ç°ç¬¬ä¸€ä¸ªæ‰‹å†™æ•°å­—ç”Ÿæˆçš„GANç½‘ç»œã€‚**

ä¸ºä¾¿äºè¯´æ˜GANçš„å…³é”®ç¯èŠ‚ï¼Œè¿™é‡Œæˆ‘ä»¬å¼±åŒ–äº†ç½‘ç»œå’Œæ•°æ®é›†çš„å¤æ‚åº¦ã€‚æ•°æ®é›†ä¸º MNISTã€ç½‘ç»œç”¨å…¨è¿æ¥å±‚ã€‚åç»­å°†ç”¨ä¸€äº›å·ç§¯å±‚çš„å®ä¾‹æ¥è¯´æ˜ã€‚

### å¯¼å…¥ç›¸å…³åº“

é¦–å…ˆå¯¼å…¥numpyã€torchç­‰æ¨¡å—ã€‚

``` python
import os
import torch
import torchvision
import torch.nn as nn
from torchvision import transforms
from torchvision.utils import save_image
```

### æ•°æ®åŠ è½½å’Œå‚æ•°å®šä¹‰

pytorchå†…ç½®é›†æˆäº†MNISTæ•°æ®é›†ã€‚

``` python
# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyper-parameters 
latent_size = 64  
hidden_size = 256
image_size = 784
num_epochs = 200
batch_size = 100
sample_dir = 'samples'

# Create a directory if not exists
if not os.path.exists(sample_dir):
    os.makedirs(sample_dir)

# Image processing
transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=0.5,std=0.5)])

# MNIST dataset
mnist = torchvision.datasets.MNIST('./data',
                                   train=True,
                                   transform=transform,
                                   download=True)

# Data loader
data_loader = torch.utils.data.DataLoader(dataset=mnist,
                                          batch_size=batch_size, 
                                          shuffle=True)
```

### åˆ¤åˆ«å™¨æ¨¡å‹

å®šä¹‰åˆ¤åˆ«å™¨ç½‘ç»œç»“æ„ï¼Œè¿™é‡Œä½¿ç”¨LeakyReLUä¸ºæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºä¸€ä¸ªèŠ‚ç‚¹å¹¶ç»è¿‡ Sigmoidåè¾“å‡ºï¼Œç”¨äºçœŸå‡äºŒåˆ†ç±»ã€‚ 

``` python
# æ„å»ºåˆ¤æ–­å™¨ 
D = nn.Sequential( 
	nn.Linear(image_size, hidden_size), 
	nn.LeakyReLU(0.2), 
	nn.Linear(hidden_size, hidden_size), 
	nn.LeakyReLU(0.2), 
	nn.Linear(hidden_size, 1), 
	nn.Sigmoid())

D = D.to(device)
```

### ç”Ÿæˆå™¨æ¨¡å‹

ä½¿ç”¨nn.tanhå°†ä½¿æ•°æ®åˆ†å¸ƒ åœ¨[-1,1]ä¹‹é—´ã€‚å…¶è¾“å…¥æ˜¯æ½œåœ¨ç©ºé—´çš„å‘é‡zï¼Œè¾“å‡ºç»´åº¦ä¸çœŸå›¾åƒç›¸åŒã€‚

``` python
# æ„å»ºç”Ÿæˆå™¨
G = nn.Sequential(
	nn.Linear(latent_size, hidden_size), 
	nn.ReLU(), 
	nn.Linear(hidden_size, hidden_size), 
	nn.ReLU(), 
	nn.Linear(hidden_size, image_size), 
	nn.Tanh())

G = G.to(device)
```

### è®­ç»ƒæ¨¡å‹

``` python
# å®šä¹‰æŸå¤±å‡½æ•°ä»¥åŠä¼˜åŒ–å™¨
# äºŒå€¼äº¤å‰ç†µæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨è®¾ç½®
# Binary cross entropy loss and optimizer
criterion = nn.BCELoss()
d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)
g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)

# è§„èŒƒåŒ–å¤„ç†
# åœ¨Image processingä¸­ä»¥mean,std=0.5è¿›è¡Œè§„èŒƒåŒ–,out=2*input-1
# æ•…input=(out+1)/2
def denorm(x):
    out = (x + 1) / 2
    # å°†outå¼ é‡æ¯ä¸ªå…ƒç´ çš„èŒƒå›´é™åˆ¶åˆ°åŒºé—´ [min,max]
    return out.clamp(0, 1)
   
# æ¢¯åº¦æ¸…é›¶    
def reset_grad():
    d_optimizer.zero_grad()
    g_optimizer.zero_grad()

# Start training
total_step = len(data_loader)
for epoch in range(num_epochs):
	for i, (images, _) in enumerate(data_loader):
		images = images.reshape(batch_size, -1).to(device) 
		
		# å®šä¹‰å›¾åƒæ˜¯çœŸæˆ–å‡çš„æ ‡ç­¾ 			
		real_labels = torch.ones(batch_size, 1).to(device) 
		fake_labels = torch.zeros(batch_size, 1).to(device) 
    	# ================================================================== # 
    	#                          è®­ç»ƒåˆ¤åˆ«å™¨  K=1                            # 
    	# ================================================================== # 
    	# å®šä¹‰åˆ¤åˆ«å™¨å¯¹çœŸå›¾åƒçš„æŸå¤±å‡½æ•°
    	outputs = D(images) # çœŸå›¾åƒè¾“å…¥ç»™é‰´åˆ«å™¨ï¼Œå¹¶äº§ç”Ÿåˆ¤åˆ«å™¨è¾“å‡º
    	# è®¡ç®—ç”±çœŸå›¾åƒè¾“å…¥ç»™åˆ¤åˆ«å™¨äº§ç”Ÿçš„è¾“å‡ºä¸çœŸçš„labelé—´çš„äºŒå€¼äº¤å‰ç†µæŸå¤±
    	d_loss_real = criterion(outputs, real_labels)
    	real_score = outputs 

    	# å®šä¹‰åˆ¤åˆ«å™¨å¯¹å‡å›¾åƒï¼ˆå³ç”±æ½œåœ¨ç©ºé—´ç‚¹ç”Ÿæˆçš„å›¾åƒï¼‰çš„æŸå¤±å‡½æ•° 
    	z = torch.randn(batch_size, latent_size).to(device) 
    	# éšæœºå‘é‡è¾“å…¥ç»™ç”Ÿæˆå™¨ï¼Œå¹¶äº§ç”Ÿç”Ÿæˆå™¨è¾“å‡ºå‡å›¾åƒ
    	fake_images = G(z)
    	outputs = D(fake_images) # å‡å›¾åƒè¾“å…¥ç»™é‰´åˆ«å™¨ï¼Œå¹¶äº§ç”Ÿåˆ¤åˆ«å™¨è¾“å‡º
    	# è®¡ç®—ç”±å‡å›¾åƒè¾“å…¥ç»™åˆ¤åˆ«å™¨äº§ç”Ÿçš„è¾“å‡ºä¸è™šçš„labelé—´çš„äºŒå€¼äº¤å‰ç†µæŸå¤±
    	d_loss_fake = criterion(outputs, fake_labels)
    	fake_score = outputs

    	# å¾—åˆ°åˆ¤åˆ«å™¨æ€»çš„æŸå¤±å‡½æ•°
    	d_loss = d_loss_real + d_loss_fake 

    	# å¯¹ç”Ÿæˆå™¨ã€åˆ¤åˆ«å™¨çš„æ¢¯åº¦æ¸…é›¶
    	# è¿›è¡Œåå‘ä¼ æ’­åŠè¿è¡Œåˆ¤åˆ«å™¨çš„ä¼˜åŒ–å™¨ 
    	reset_grad()
    	d_loss.backward()
    	d_optimizer.step() 
    	# ================================================================== # 
    	#                           è®­ç»ƒç”Ÿæˆå™¨                                # 
    	# ================================================================== # 
    	# å®šä¹‰ç”Ÿæˆå™¨å¯¹å‡å›¾åƒçš„æŸå¤±å‡½æ•°
    	z = torch.randn(batch_size, latent_size).to(device)
        # éšæœºå‘é‡è¾“å…¥ç»™ç”Ÿæˆå™¨ï¼Œå¹¶äº§ç”Ÿç”Ÿæˆå™¨è¾“å‡ºå‡å›¾åƒ
    	fake_images = G(z)
    	outputs = D(fake_images) # å‡å›¾åƒè¾“å…¥ç»™é‰´åˆ«å™¨ï¼Œå¹¶äº§ç”Ÿåˆ¤åˆ«å™¨è¾“å‡º
		# è®¡ç®—ç”Ÿæˆå™¨ç”Ÿæˆçš„å‡å›¾è¾“å…¥ç»™é‰´åˆ«å™¨é‰´åˆ«è¾“å‡ºä¸çœŸçš„labelé—´çš„äºŒå€¼äº¤å‰ç†µæŸå¤±
    	g_loss = criterion(outputs, real_labels) 

    	# å¯¹ç”Ÿæˆå™¨ã€åˆ¤åˆ«å™¨çš„æ¢¯åº¦æ¸…é›¶ 
    	# è¿›è¡Œåå‘ä¼ æ’­åŠè¿è¡Œç”Ÿæˆå™¨çš„ä¼˜åŒ–å™¨ 
    	reset_grad()
    	g_loss.backward()
    	g_optimizer.step()
		# æ¯è¿­ä»£ä¸€å®šæ­¥éª¤ï¼Œæ‰“å°ç»“æœå€¼
    	if (i + 1) % 200 == 0:
            print(
                "Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}".format(
                    epoch,
                    num_epochs,
                    i + 1,
                    total_step,
                    d_loss.item(),
                    g_loss.item(),
                    real_score.mean().item(),
                    fake_score.mean().item(),
                )
            )

    # ä¿å­˜çœŸå›¾åƒ
    if (epoch + 1) == 1:
      	images = images.reshape(images.size(0), 1, 28, 28)
      	save_image(denorm(images), os.path.join(sample_dir, 'real_images.png')) 
      
    # ä¿å­˜å‡å›¾åƒ 
    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28) 
    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1))) 

# ä¿å­˜æ¨¡å‹
torch.save(G.state_dict(), 'G.ckpt')
torch.save(D.state_dict(), 'D.ckpt')
```

### å¯è§†åŒ–ç»“æœ

```
import matplotlib.image as mpimg
import matplotlib.pyplot as plt

reconsPath = './samples/fake_images-200.png' 
Image = mpimg.imread(reconsPath) 
plt.imshow(Image)
plt.axis('off')
plt.show() 
```

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter05/fake_images.png" alt="fake_images" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter05/iter_img.png" alt="æ‰‹å†™æ•°å­—å›¾ç‰‡" style="zoom: 50%;" />

### å°ç»“

åœ¨æœ¬èŠ‚å†…å®¹ä¸­ï¼Œæˆ‘ä»¬ä»ä»£ç çš„è§’åº¦ç›´è§‚æ¸…æ™°çš„äº†è§£äº†æŸå¤±å‡½æ•°ä»¥åŠæ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„pytorchå®ç°è¿‡ç¨‹ã€‚åŠ¨æ‰‹äº²è‡ªå®Œæˆäº†æ‰‹å†™æ•°å­—ç”Ÿæˆçš„GANé¡¹ç›®å®è·µã€‚è™½ç„¶è¿™ä¸ªé¡¹ç›®å¯èƒ½åœ¨è¯»è€…çœ‹èµ·æ¥æ¯”è¾ƒç®€å•ï¼Œä½†å´æ˜¯æ•´ä¸ªGANæ¨¡å‹çš„æ ¸å¿ƒæ‰€åœ¨ã€‚è€Œä¾‹å¦‚CGANï¼ŒDCGANï¼ŒCycleGANç­‰ç­‰ä¹Ÿéƒ½æ˜¯åŸºäºè¿™æ®µç®€å•çš„ä»£ç ï¼Œåœ¨ç½‘ç»œå±‚æ–¹é¢è¿›è¡Œæ”¹è¿›ï¼Œå½¢æˆæ–°çš„æ¨¡å‹ã€‚  
